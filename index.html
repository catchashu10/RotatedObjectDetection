<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="To create a simple yet effective approach for fast and accurate rotated object detection in remote sensing images.​">
  <meta property="og:title" content="rotated-object-detection-in-remote-sensing-images" />
  <meta property="og:description"
    content="To create a simple yet effective approach for fast and accurate rotated object detection in remote sensing images.​" />
  <meta property="og:url" content="https://catchashu10.github.io/RotatedObjectDetection/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="rotated-object-detection-in-remote-sensing-images">
  <meta name="twitter:description"
    content="To create a simple yet effective approach for fast and accurate rotated object detection in remote sensing images.​">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="rotated-object-detection remote-sensing dota ucas-aod yolo rcnn">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Rotated Object Detection in Remote Sensing Images</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon_object.webp">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <!-- Background for the Entire Page -->
  <section class="hero" style="background-image: url('static/images/backgrounduw_gray.jpeg'); background-size: cover; background-position: center; background-attachment: fixed;">
  <div class="hero-body" style="background-color: rgba(255, 255, 255, 0.2); padding: 20px;">

  <!-- Header Section With Background -->
  <section class="hero" style="background-image: url('static/images/backgrounduw.jpg'); background-size: cover; background-position: center; background-attachment: fixed;">
    <div class="hero-body" style="background-color: rgba(255, 255, 255, 0.1); padding: 20px;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Rotated Object Detection in Remote Sensing Images</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/catchashu10" target="_blank">Ashu Kumar</a>,</span>
              <span class="author-block">
                <a href="" target="_blank">Neeraj Menon</a>,</span>
              <span class="author-block">
                <a href="" target="_blank">Pooja Yadav</a>,</span>
              <span class="author-block">
                <a href="https://pratiksanghavi.in/" target="_blank">Pratik Sanghavi</a>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">Instructor - Pedro Morgado</span><br> -->
              <span class="author-block">CS 766 - Computer Vision</span><br>
              <span class="author-block">University of Wisconsin-Madison<br>2024</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/CS766_Computer_Vision_Project_Proposal.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Project Proposal</span>
                  </a>
                </span>

                <!-- Midterm Report -->
                <span class="link-block">
                  <a href="static/pdfs/CS766_Computer_Vision_Midterm_Report.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Midterm Report</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/catchashu10/RotatedObjectDetection" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Presentation Link -->
                <span class="link-block">
                  <a href="#presentation" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Presentation</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Header Section Ends-->

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Our work explores advanced techniques for detecting rotated objects within complex remote sensing imagery.
              Significant challenges such as arbitrary orientations, extensive aspect ratios, and dense object
              distributions make this an intractable problem. Our study assesses the performance of two primary object
              detection frameworks: two-stage and single-stage detectors. Introducing an innovative approach that
              integrates MMRotate, a specialized toolbox for rotated object detection, the analysis includes a two-stage
              detector utilizing an oriented RCNN model. Concurrently, it implements a single-stage detector based on
              YOLOv7, modified with the loss function from R3Det to support arbitrary orientations. Evaluations were
              performed on two different datasets: UCAS-AOD, which focuses on planes and vehicles, and DOTA, which
              includes a more varied set of categories. Our results demonstrated the efficacy of single stage detectors
              with respect to slower two stage models. Future directions will explore oversampling techniques to balance
              class representation in the DOTA dataset and further enhance detection precision.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Motivation -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Motivation</h2>
          <div class="content has-text-justified">
            <p>
              Rotated object detection, or arbitrary-oriented object detection, is the process of identifying and
              locating objects within images that may be oriented in any direction. This task is particularly
              challenging because objects can appear in varying orientations, even within the same image, and are often
              set against complex backgrounds, densely packed together, or obscured by occlusions. Unlike traditional
              object detection that assumes objects align with the image axes, rotated object detection must handle
              objects that are arbitrarily oriented—a common scenario in natural scenes.
            </p>
            <p>
              This complexity is often seen in areas like remote sensing, text detection in natural settings, face
              recognition, retail environments, medical imaging, and industrial inspections. Standard backbone network
              models, designed primarily for axis-aligned object detection, struggle with the unique demands of
              detecting rotated objects. These models frequently fail to capture high-quality features of such objects
              due to their architecture not accounting for arbitrary orientations.
            </p>
            <p>
              The advantages of rotated object detection over traditional methods are clear: it typically includes less
              background within the detection box and can provide additional information through the orientation of the
              object, which may indicate its pose.
            </p>
            <p>
              Given its wide range of applications—including autonomous vehicles, sports analysis, manufacturing quality
              control, satellite imagery, and urban planning—enhancing rotated object detection is crucial. This project
              aims to tackle the inherent challenges in this area, striving for a detection method that surpasses the
              limitations of axis-aligned models and better suits the varied orientations of objects in real-world
              scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper background -->

  <!-- Related Work -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Exploration of Related Work in Object Detection Technologies</h2>
          <div class="content has-text-justified">
            <p>
              In the evolving landscape of object detection, current methodologies can be broadly categorized into two
              distinct types: two-stage detectors and single-stage detectors. Each category utilizes unique techniques
              and algorithms to address the intricate challenges of object detection, catering to different needs in
              terms of speed and accuracy.
            </p>
            <div class="flex-container" style="display: flex; flex-wrap: wrap; justify-content: space-between;">
              <div class="flex-item" style="flex: 1 1 48%; margin-right: 4%;">
                <h3 class="title is-4">Two-Stage Object Detectors</h3>
                <p>
                  Two-stage object detectors exemplify a refined approach where accuracy is paramount. These methods are
                  characterized by their sequential processing where initial regions of interest are proposed and then
                  refined through classification and localization.
                </p>
                <p>
                  The process begins with the generation of candidate regions through specialized modules, which are
                  meticulously crafted to propose potential areas of interest. This is followed by a rigorous
                  classification stage, wherein each candidate is evaluated and classified. Finally, non-maximal
                  suppression techniques are employed to eliminate redundant detections, ensuring that each object is
                  uniquely identified.
                </p>
                <p>
                  Esteemed examples of this methodology include the R-CNN series, encompassing the original R-CNN, Fast
                  R-CNN, and the Faster R-CNN, each iteration building on its predecessor to enhance both speed and
                  accuracy.
                </p>
              </div>
              <div class="flex-item" style="flex: 1 1 48%;">
                <h3 class="title is-4">Single-Stage Object Detectors</h3>
                <p>
                  In contrast to their two-stage counterparts, single-stage object detectors prioritize speed, enabling
                  real-time processing capabilities. These detectors streamline the detection process by eliminating the
                  proposal generation phase.
                </p>
                <p>
                  This methodology leverages the concept of direct target regression and requires only a single pass
                  through the neural network to extract features and detect objects. The simplicity of this approach
                  allows for rapid detection speeds, albeit typically at a compromise in accuracy compared to two-stage
                  methods.
                </p>
                <p>
                  Prominent models within this category include YOLO (You Only Look Once) and SSD (Single Shot MultiBox
                  Detector), both of which are celebrated for their efficiency and are widely utilized in applications
                  requiring real-time processing.
                </p>
              </div>
            </div>
            <div class="image-container">
              <img src="static/images/rcnns.png" alt="RCNNs architecture">
              <div class="caption">Architecture of 2 stage RCNNs</div>
            </div>
            <div class="image-container">
              <img src="static/images/single_stage_detectors.png" alt="Single stage model architecture">
              <div class="caption">Architecture of single stage YOLO model</div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Methodology -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Methodology</h2>
          <div class="content has-text-justified">
            <p>
              We now know the context around oriented bounding box detection. So how would we go around solutioning for
              this problem statement? Luckily we have a plethora of models to choose and adapt from. The catch? Like
              everything else in life there's no free meal here as well!​
            </p>
            <div class="baseline">
              <h3>Baseline establishment with Oriented RCNNs</h3>
              <p>
                We want to have a robust and stable baseline. For this, we chose oriented RCNN since it is one of the
                state of the art models that is consistently used as a baseline in some of the more recent papers.
              </p>
              <div class="image-container">
                <img src="static/images/oriented_rcnns.jpeg" alt="Oriented RCNNS">
                <div class="caption">Architecture details of Oriented RCNNs</div>
              </div>
              <p>
                We used the implementation provided by MMRotate, a toolbox that builds on PyTorch and is geared towards
                oriented bounding box training and inference. MMRotate consists of 4 main parts, datasets, models, core
                and apis.
              </p>
              <ul>
                <li>"datasets" is for data loading and data augmentation. In this part, we support various datasets for
                  rotated object detection algorithms, useful data augmentation transforms in pipelines for
                  pre-processing image.</li>
                <li>"models" contains models and loss functions.</li>
                <li>"core" provides evaluation tools for model training and evaluation.</li>
                <li>"apis" provides high-level APIs for models training, testing, and inference.</li>
              </ul>
              <div class="image-container">
                <img src="static/images/mmrotate_framework.png" alt="MMRotate">
                <div class="caption">Module design of MMRotate</div>
              </div>
              <p>Now we observed some stellar results on some classes with large confidence values in our reference
                dataset but the mean average precision or mAP - which is the figure of merit we will be using to measure
                our performance tapered off pretty quickly on some of the sparser classes.​</p>
            </div>
            <div class="singe-stage-detector">
              <h3>Single Stage Detector based on YOLOv7</h3>
              <p>
                Single stage models have seen significant strides in recent times. The real-time nature of inference and
                small footprint is desirable in many use cases. The lower mAP value is something the research community
                has been focussing on and iteratively improving upon.
              </p>
              <p>
                We combine YOLOv7 model and loss function from R3Det for arbitrarily-oriented object detection. The loss
                function is given by
              <div class="formula">
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <mi>L</mi>
                  <mo>=</mo>
                  <msub>
                    <mi>&lambda;</mi>
                    <sub>
                      <mi>coord</mi>
                    </sub>
                  </msub>
                  <mi>L</mi><sub>reg</sub>
                  <mo>+</mo>
                  <msub>
                    <mi>&lambda;</mi>
                    <sub>
                      <mi>conf</mi>
                    </sub>
                  </msub>
                  <mi>L</mi><sub>conf</sub>
                  <mo>+</mo>
                  <msub>
                    <mi>&lambda;</mi>
                    <sub>
                      <mi>cls</mi>
                    </sub>
                  </msub>
                  <mi>L</mi><sub>cls</sub>
                  <mo>+</mo>
                  <msub>
                    <mi>&lambda;</mi>
                    <sub>
                      <mi>theta</mi>
                    </sub>
                  </msub>
                  <mi>L</mi><sub>theta</sub>
                </math>
              </div>
              <p>Where:</p>
              <ul>
                <li><math>
                    <mi>L</mi><sub>reg</sub>
                  </math> is the regression loss, computed as the mean of (1 - CIoU) for each bounding box.</li>
                <li><math>
                    <mi>L</mi><sub>conf</sub>
                  </math> is the confidence loss, using binary cross-entropy with or without focal loss depending on
                  objectness scores.</li>
                <li><math>
                    <mi>L</mi><sub>cls</sub>
                  </math> is the class loss, using binary cross-entropy with or without focal loss based on class
                  predictions.</li>
                <li><math>
                    <mi>L</mi><sub>theta</sub>
                  </math> is the theta loss, using binary cross-entropy with or without focal loss based on the theta
                  predictions.</li>
              </ul>
              </p>
              <div class="image-container">
                <img src="static/images/yolov7_with_loss.png" alt="YOLOv7 with R3Det loss">
                <div class="caption">YOLOv7 block diagram with custom loss function</div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of methodology -->

  <!-- The Datasets -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The Datasets</h2>
          <div class="content has-text-justified">
            <p>
              Our research utilizes two primary datasets widely recognized within the object detection community. These
              datasets, UCAS-AOD and DOTA, offer a rich collection of images and annotations that are essential for
              training and evaluating our models.
            </p>
            <div class="flex-container" style="display: flex; flex-wrap: wrap; justify-content: space-between;">
              <div class="flex-item" style="flex: 1 1 48%; margin-right: 4%;">
                <h3 class="title is-4">UCAS-AOD Dataset</h3>
                <ul>
                  <li>One of the earliest datasets to use oriented bounding boxes</li>
                  <li>Comprises 1500 images featuring two specific categories: planes and vehicles</li>
                  <li>Designed to facilitate advances in object detection algorithms</li>
                </ul>
                <div class="image-container">
                  <img src="static/images/ucas.png" alt="UCAS-AOD Dataset">
                  <div class="caption">Instances per class label in UCAS Dataset</div>
                </div>
              </div>
              <div class="flex-item" style="flex: 1 1 48%;">
                <h3 class="title is-4">DOTA Dataset</h3>
                <ul>
                  <li>Targets object detection in aerial imagery (DOTA)</li>
                  <li>Features a diverse set of 15 categories</li>
                  <li>Encompasses 2806 images with 1,88,282 bounding boxes</li>
                  <li>Unbalanced instances of classes</li>
                </ul>
                <div class="image-container">
                  <img src="static/images/dota.png" alt="DOTA Dataset">
                  <div class="caption">Instances per class label in DOTA Dataset</div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of datasets -->

  <!-- Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Evaluation Results</h2>
          <div class="content has-text-justified">
            <p>
              Our study leverages two prominent datasets within the object detection field, UCAS-AOD and DOTA. These
              datasets are pivotal for the development and validation of our object detection models, providing
              extensive image resources and annotations.
            </p>
            <div class="flex-container" style="display: flex; flex-wrap: wrap; justify-content: space-between;">
              <div class="flex-item" style="flex: 1 1 48%; margin-right: 4%; display: flex; flex-direction: column;">
                <h3 class="title is-4">UCAS-AOD Dataset Performance</h3>
                <p>
                  Our model achieved significant improvements on the UCAS-AOD dataset, recording an overall mean Average
                  Precision (mAP) of 0.95. This demonstrates superior performance compared to traditional two-stage
                  models.
                </p>
                <div class="image-container" style="margin-bottom: 20px;">
                  <img src="static/images/ucas_result.png" alt="Metrics for UCAS-AOD Dataset"
                    style="width: 100%; height: auto;">
                  <div class="caption">Performance metrics for UCAS-AOD Dataset</div>
                </div>
                <p>
                  Our single-stage model not only simplified the processing pipeline but also matched or exceeded the
                  performance metrics of conventional two-stage models for the UCAS dataset.
                </p>
                <div class="image-container">
                  <img src="static/images/ucas_result_otherApproaches.png"
                    alt="Comparison of our single-stage approach with other 2 stage approaches for UCAS Dataset"
                    style="width: 100%; height: auto;">
                  <div class="caption">Comparison with other two-stage models</div>
                </div>
              </div>
              <div class="flex-item" style="flex: 1 1 48%; display: flex; flex-direction: column;">
                <h3 class="title is-4">DOTA Dataset Insights</h3>
                <p>
                  The DOTA dataset revealed a higher mean Average Precision for detecting planes, cars, and ships.
                  However, performance varied across different classes, reflecting challenges due to class imbalance.
                </p>
                <div class="image-container">
                  <img src="static/images/dota_result.png" alt="Metrics for DOTA Dataset"
                    style="width: 100%; height: auto;">
                  <div class="caption">Performance metrics for DOTA Dataset</div>
                </div>
                <p>
                  Further insights from the DOTA dataset highlight the need for more nuanced approaches to tackle the
                  lower detection accuracies in less represented classes. Future research efforts aim to refine our
                  model's performance across these challenging scenarios.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of results -->

  <!-- Demo/Outputs -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Demo / Outputs</h2>
          <div class="content has-text-justified">
            <!-- Image grid -->
            <section class="hero is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-grid" class="image-grid">
                    <!-- Images will be loaded here by JavaScript -->
                  </div>
                </div>
              </div>
            </section>
            <!-- End image grid -->
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <!-- Youtube embed code here -->
              <iframe src="https://www.youtube.com/embed/GMXJ1vagsgY?si=6B6xMtmQGeFwpr4Q" frameborder="0"
                allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <!-- Presentation -->
  <section class="hero is-small is-light" id="presentation">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Presentation</h2>
        <iframe
          src="https://uwprod-my.sharepoint.com/personal/kumar273_wisc_edu/_layouts/15/Doc.aspx?sourcedoc={440e818a-e7d1-4084-81c1-248726773053}&amp;action=embedview&amp;wdAr=1.7777777777777777"
          width="100%" height="550px" frameborder="0">This is an embedded <a target="_blank"
            href="https://office.com">Microsoft Office</a> presentation, powered by <a target="_blank"
            href="https://office.com/webapps">Office</a>.</iframe>

      </div>
    </div>
  </section>
  <!--End presentation -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>
        @article{Ding2021,
          title={Object Detection in Aerial Images: A Large-Scale Benchmark and Challenges},
          author={Ding, Jian and others},
          journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
          volume={2021},
          pages={1--1},
          year={2021},
          doi={10.1109/TPAMI.2021}
        }
        
        @inproceedings{Xie2021,
          title={Oriented R-CNN for Object Detection},
          author={Xie, Xingxing and others},
          booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
          pages={3520--3529},
          year={2021},
          month={Oct}
        }
        
        @inproceedings{Zhou2022,
          title={MMRotate: A Rotated Object Detection Benchmark using PyTorch},
          author={Zhou, Yue and others},
          booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
          year={2022}
        }
        
        @inproceedings{Yang2021,
          title={R3det: Refined single-stage detector with feature refinement for rotating object},
          author={Yang, X. and others},
          booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
          volume={35},
          number={4},
          pages={3163--3171},
          year={2021},
          doi={10.1609/aaai.v35i4.16426}
        }
        @InProceedings{Xie_2021_ICCV,
          author = {Xie, Xingxing and Cheng, Gong and Wang, Jiabao and Yao, Xiwen and Han, Junwei},
          title = {Oriented R-CNN for Object Detection},
          booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
          month = {October},
          year = {2021},
          pages = {3520-3529} 
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
      <div>
            <p>
              Thank you for visiting our project page!
            </p>
      </div>
    </div>
  </footer>

</body>
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const gridContainer = document.getElementById('results-grid');
    const imageFolder = 'static/images/outputs/'; // Path to the folder

    for (let i = 1; i <= 36; i++) {
      const img = document.createElement('img');
      img.src = `${imageFolder}${i}.png`;
      img.alt = `Image ${i}`;
      img.className = 'grid-image';
      gridContainer.appendChild(img);
    }
  });
</script>

</html>